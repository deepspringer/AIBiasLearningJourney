export const ALGORITHMIC_BIAS_TEXT = [
  '## How Large Language Models Work\n\nImagine your phone\'s autocomplete feature that suggests words as you type. **Large Language Models (LLMs)** like ChatGPT work similarly, but on a much larger scale. They predict what word should come next based on all the text they\'ve read.\n\nFor example, if you write "I like to eat ice..." the AI might predict "cream" should follow. These predictions happen token by token (tokens are words or word parts like prefixes), with each possibility assigned a probability score. While the AI usually selects the most likely token, it includes some randomness to avoid being too predictable.',
  
  "## Learning from Human Biases\n\nSince LLMs learn from human-written text from the internet and books, they can pick up the same biases that exist in our society. If certain groups of people are described in stereotypical ways in the training data, the AI might learn and repeat these patterns unless steps are taken to prevent this.",
  
  "## Real-World Research\n\nScientists have conducted experiments to study these biases. In one study called \"**The Silicon Ceiling**,\" researchers created identical resumes but gave them different names suggesting different genders and racial backgrounds (like \"Jermaine Jackson\" versus \"Matthew Owens\").\n\nWhen asked to score these resumes, the AI sometimes rated them differently based solely on the names. Even more surprisingly, when asked to create resumes for fictional people with different names, the AI made assumptions about their backgrounds—creating resumes with:\n\n- Less work experience for women\n- \"Immigrant markers\" for Asian and Hispanic names\n\nAll without being told anything about these fictional people!\n\n### Names Used in the Study\n\n**Black or African American women's names:**\n- Keisha Towns\n- Tyra Cooks\n- Janae Washington\n- Monique Rivers\n\n**Black or African American men's names:**\n- Jermaine Jackson\n- Denzel Gaines\n- Darius Mosby\n- Darnell Dawkins\n\n**Hispanic or Latinx American women's names:**\n- Maria Garcia\n- Vanessa Rodriguez\n- Laura Ramirez\n- Gabriela Lopez\n\n**Hispanic or Latinx American men's names:**\n- Miguel Fernandez\n- Christian Hernandez\n- Joe Alvarez\n- Rodrigo Romero\n\n**Asian American women's names:**\n- Vivian Cheng\n- Christina Wang\n- Suni Tran\n- Mei Lin\n\n**Asian American men's names:**\n- George Yang\n- Harry Wu\n- Pheng Chan\n- Kenji Yoshida\n\n**White American women's names:**\n- Katie Burns\n- Cara O'Connor\n- Allison Baker\n- Meredith Rogers\n\n**White American men's names:**\n- Gregory Roberts\n- Matthew Owens\n- Paul Bennett\n- Chad Nichols",
  
  '## Cultural Variations in Bias\n\nBias also varies across cultures. Another study called "**IndiBias**" found that AI systems developed in the United States might be trained to avoid biases common in American society but miss biases specific to other cultures.\n\nThey tested AI systems for biases related to Indian society, including categories like:\n\n- Gender\n- Religion\n- Caste\n- Region\n\nThis highlights how important it is for AI to understand diverse cultural contexts from around the world, not just perspectives common in the United States.',
  
  "## Working Toward Solutions\n\nResearchers are working on solutions to reduce bias in AI. These include:\n\n1. Using diverse training data that represents many perspectives\n2. Creating tests to check for biases before releasing AI systems\n3. Fine-tuning the AI with carefully selected examples to reduce biases\n4. Having people from diverse backgrounds review AI outputs\n5. Involving people from different countries in the AI development process\n\n> \"Understanding bias in AI is important because these systems increasingly influence many areas of our lives—from healthcare decisions to college admissions.\"\n\n![Bias in AI](https://i.imgur.com/example.jpg)",
];

export const OLD_ALGORITHMIC_BIAS_TEXT = [
  "Algorithmic bias refers to systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others. Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected, or used to train the algorithm.",
  'Machine learning algorithms learn to make decisions based on training data, which can include biased human decisions or reflect historical or social inequities, even if sensitive variables such as gender, race, or sexual orientation are removed. This is known as "bias in, bias out" - when algorithms are trained on data containing human biases, they can amplify those biases in their outputs.',
  "For example, an AI hiring tool trained on historical hiring data might learn to prefer male candidates for technical roles if that reflects past hiring patterns. Similarly, facial recognition systems may perform better on lighter-skinned faces if they were primarily trained on datasets with underrepresentation of darker-skinned individuals.",
  'Addressing algorithmic bias requires diverse datasets, regular auditing for fairness, transparent AI systems, and diverse teams developing these technologies. Some researchers advocate for "algorithmic fairness" - designing systems that produce fair and equitable outcomes across different demographic groups.',
  "As AI systems become more integrated into society, understanding and mitigating algorithmic bias is crucial. Researchers, companies, and policymakers are increasingly working on methods to detect and reduce bias in algorithms to ensure these technologies benefit everyone fairly.",
];
